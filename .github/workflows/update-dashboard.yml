name: Update Publications Dashboard

on:
  schedule:
    # æ¯æ—¥ãƒ‘ãƒªæ™‚é–“7:00ï¼ˆUTC 5:00ï¼‰ã«è‡ªå‹•å®Ÿè¡Œ
    - cron: '0 5 * * *'
  workflow_dispatch:

jobs:
  crawl-and-update:
    runs-on: ubuntu-latest
    timeout-minutes: 15  # 15åˆ†ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
    
    permissions:
      contents: write
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v3
      
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Install dependencies
      run: |
        pip install requests beautifulsoup4 pytz lxml
        
    - name: Run safe crawler
      run: |
        python3 -c "
        import requests
        import json
        import re
        from datetime import datetime
        from bs4 import BeautifulSoup
        import pytz
        import time
        import sys
        
        # ãƒ‘ãƒªæ™‚é–“è¨­å®š
        paris_tz = pytz.timezone('Europe/Paris')
        current_time = datetime.now(paris_tz).strftime('%Y-%m-%d %H:%M:%S (ãƒ‘ãƒªæ™‚é–“)')
        
        print(f'ğŸš€ ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°é–‹å§‹: {current_time}')
        
        # å®‰å…¨ãªã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°è¨­å®š
        session = requests.Session()
        session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        # ç°¡å˜ãªæ©Ÿé–¢ãƒªã‚¹ãƒˆï¼ˆæ®µéšçš„ã«æ‹¡å¼µï¼‰
        institutions = [
            {
                'name': 'EBA',
                'url': 'https://www.eba.europa.eu/',
                'publication': 'ESEP',
                'search_terms': ['esep', 'supervisory review']
            },
            {
                'name': 'IMF',
                'url': 'https://www.imf.org/en/Publications',
                'publication': 'World Economic Outlook',
                'search_terms': ['world economic outlook', 'weo']
            },
            {
                'name': 'OECD',
                'url': 'https://www.oecd.org/en/publications/',
                'publication': 'Economic Outlook',
                'search_terms': ['economic outlook']
            }
        ]
        
        results = []
        success_count = 0
        
        for inst in institutions:
            try:
                print(f'ğŸ” {inst[\"name\"]} ã‚’ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°ä¸­...')
                
                # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ããƒªã‚¯ã‚¨ã‚¹ãƒˆ
                response = session.get(inst['url'], timeout=10)
                if response.status_code == 200:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # ç°¡å˜ãªæ¤œç´¢
                    found_content = False
                    for term in inst['search_terms']:
                        if term.lower() in response.text.lower():
                            found_content = True
                            break
                    
                    results.append({
                        'institution': inst['name'],
                        'publication': inst['publication'],
                        'status': 'âœ… æ¥ç¶šæˆåŠŸ' if found_content else 'âš ï¸ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æœªç¢ºèª',
                        'crawled_at': current_time
                    })
                    success_count += 1
                    print(f'âœ… {inst[\"name\"]} å®Œäº†')
                else:
                    results.append({
                        'institution': inst['name'],
                        'publication': inst['publication'],
                        'status': f'âŒ HTTP {response.status_code}',
                        'crawled_at': current_time
                    })
                    print(f'âŒ {inst[\"name\"]} å¤±æ•—: HTTP {response.status_code}')
                
                # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ï¼šå„ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã«3ç§’å¾…æ©Ÿ
                time.sleep(3)
                
            except Exception as e:
                results.append({
                    'institution': inst['name'],
                    'publication': inst['publication'],
                    'status': f'âŒ ã‚¨ãƒ©ãƒ¼: {str(e)[:50]}',
                    'crawled_at': current_time
                })
                print(f'âŒ {inst[\"name\"]} ã‚¨ãƒ©ãƒ¼: {e}')
                continue
        
        # çµæœä¿å­˜
        with open('crawl_results.json', 'w', encoding='utf-8') as f:
            json.dump({
                'last_crawl': current_time,
                'total_institutions': len(institutions),
                'successful_crawls': success_count,
                'results': results
            }, f, ensure_ascii=False, indent=2)
        
        print(f'ğŸ“Š ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°å®Œäº†: {success_count}/{len(institutions)} æˆåŠŸ')
        "
        
    - name: Update HTML dashboard
      run: |
        # HTMLãƒ•ã‚¡ã‚¤ãƒ«ã®æœ€çµ‚æ›´æ–°æ™‚åˆ»ã‚’æ›´æ–°
        PARIS_TIME=$(TZ='Europe/Paris' date '+%Y-%m-%d %H:%M:%S (ãƒ‘ãƒªæ™‚é–“)')
        
        if [ -f "index.html" ]; then
          # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå¤‰æ•°ã‚’ç½®æ›
          sed -i "s/{{LAST_UPDATE_TIME}}/$PARIS_TIME/g" index.html
          
          # æ—¢å­˜ã®æ›´æ–°æ™‚åˆ»ã‚‚ç½®æ›
          sed -i "s/æœ€çµ‚æ›´æ–°: [^<]*/æœ€çµ‚æ›´æ–°: $PARIS_TIME/g" index.html
          
          echo "âœ… HTMLãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰æ›´æ–°å®Œäº†"
        else
          echo "âŒ index.html ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
        fi
        
    - name: Commit and push changes
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action Bot"
        
        # å¤‰æ›´ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        if [ -n "$(git status --porcelain)" ]; then
          git add index.html crawl_results.json
          git commit -m "ğŸ¤– è‡ªå‹•æ›´æ–°: $(TZ='Europe/Paris' date '+%Y-%m-%d %H:%M')"
          git push
          echo "âœ… å¤‰æ›´ã‚’ãƒ—ãƒƒã‚·ãƒ¥ã—ã¾ã—ãŸ"
        else
          echo "â„¹ï¸ å¤‰æ›´ãŒãªã„ãŸã‚ã‚³ãƒŸãƒƒãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ"
        fi
