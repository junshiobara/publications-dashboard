"""
36æ©Ÿé–¢å…¬è¡¨ç‰©ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ 
HTMLãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã¨å®Œå…¨é€£æº
"""

import requests
import schedule
import time
import json
import re
from datetime import datetime, timedelta
from pathlib import Path
from bs4 import BeautifulSoup
import pytz
import logging

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class PublicationMonitor:
    def __init__(self):
        self.paris_tz = pytz.timezone('Europe/Paris')
        self.data_file = 'publications_data.json'
        self.html_template = 'dashboard_template.html'
        self.html_output = 'index.html'
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨­å®š
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        # 36æ©Ÿé–¢ã®å…¬è¡¨ç‰©å®šç¾©ï¼ˆHTMLãƒ†ãƒ¼ãƒ–ãƒ«ã¨å®Œå…¨å¯¾å¿œï¼‰
        self.institutions = [
            {
                'id': 1,
                'institution': 'EBA',
                'publication': 'ESEP',
                'frequency': 'Annual',
                'url': 'https://www.eba.europa.eu/',
                'search_patterns': [r'esep.*report', r'supervisory.*review.*evaluation'],
                'last_published': '2024-07-08',
                'last_title': 'EBA ESEP Report 2024',
                'predicted_next': '2025å¹´7æœˆ'
            },
            {
                'id': 2,
                'institution': 'EBA',
                'publication': 'EU-wide Stress Test',
                'frequency': 'Bi-Annual',
                'url': 'https://www.eba.europa.eu/',
                'search_patterns': [r'stress.*test', r'eu.*wide.*stress'],
                'last_published': '2023-07-28',
                'last_title': 'EU-wide Stress Test Results 2023',
                'predicted_next': '2025å¹´å¾ŒåŠ'
            },
            {
                'id': 3,
                'institution': 'S&P',
                'publication': 'Portugal Sovereign Rating',
                'frequency': 'Semi-Annual',
                'url': 'https://www.spglobal.com/ratings/en/sector/governments/sovereigns',
                'search_patterns': [r'portugal.*sovereign', r'portugal.*rating'],
                'last_published': '2025-02-28',
                'last_title': 'Portugal Sovereign Rating Review',
                'predicted_next': '2025-08-29'
            },
            {
                'id': 4,
                'institution': 'Banque de France',
                'publication': 'Macroeconomic Projections',
                'frequency': 'Quarterly',
                'url': 'https://www.banque-france.fr/en/publications-and-statistics/publications',
                'search_patterns': [r'macroeconomic.*projection', r'economic.*forecast'],
                'last_published': '',
                'last_title': '',
                'predicted_next': '2025å¹´9æœˆ',
                'last_year_same_period': '2024-09-17'
            },
            {
                'id': 5,
                'institution': 'Banco de EspaÃ±a',
                'publication': 'Macroeconomic Projections',
                'frequency': 'Quarterly',
                'url': 'https://www.bde.es/wbe/en/publicaciones/',
                'search_patterns': [r'macroeconomic.*projection', r'economic.*forecast'],
                'last_published': '',
                'last_title': '',
                'predicted_next': '2025å¹´9æœˆ',
                'last_year_same_period': '2024-09-17'
            },
            {
                'id': 6,
                'institution': 'OECD',
                'publication': 'Interim Economic Outlook',
                'frequency': 'Irregular',
                'url': 'https://www.oecd.org/en/publications/',
                'search_patterns': [r'interim.*economic.*outlook', r'economic.*outlook.*interim'],
                'last_published': '2025-03-17',
                'last_title': 'OECD Interim Economic Outlook, March 2025',
                'predicted_next': '2025å¹´9æœˆ'
            },
            # ... æ®‹ã‚Šã®30æ©Ÿé–¢ã‚‚åŒæ§˜ã«å®šç¾©
            # (ã‚¹ãƒšãƒ¼ã‚¹ã®é–¢ä¿‚ã§çœç•¥ã—ã¾ã™ãŒã€å®Ÿéš›ã®ã‚³ãƒ¼ãƒ‰ã§ã¯å…¨36æ©Ÿé–¢ã‚’å«ã‚ã‚‹)
        ]
        
        # æ ¼ä»˜ã‘æ©Ÿé–¢ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆPDFãƒ™ãƒ¼ã‚¹ï¼‰
        self.rating_schedule = {
            'S&P': {
                'Spain': '2025-09-12',
                'France': '2025-11-28',
                'Italy': '2025-10-10',
                'Portugal': '2025-08-29'
            },
            'Fitch': {
                'France': '2025-09-12',
                'Portugal': '2025-09-12',
                'Italy': '2025-09-19',
                'Spain': '2025-09-26'
            },
            'Moody\'s': {
                'France': '2025-10-24',
                'Portugal': '2025-11-14',
                'Italy': '2025-11-21',
                'Spain': '2025å¹´11æœˆ'
            }
        }
        
        self.load_data()

    def load_data(self):
        """éå»ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        try:
            if Path(self.data_file).exists():
                with open(self.data_file, 'r', encoding='utf-8') as f:
                    self.historical_data = json.load(f)
            else:
                self.historical_data = {}
            logger.info(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(self.historical_data)} ä»¶")
        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            self.historical_data = {}

    def save_data(self):
        """ãƒ‡ãƒ¼ã‚¿ä¿å­˜"""
        try:
            with open(self.data_file, 'w', encoding='utf-8') as f:
                json.dump(self.historical_data, f, ensure_ascii=False, indent=2)
            logger.info("ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†")
        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    def crawl_institution(self, institution_config):
        """å€‹åˆ¥æ©Ÿé–¢ã®ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°"""
        try:
            logger.info(f"ğŸ” {institution_config['institution']} - {institution_config['publication']} ã‚’ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°ä¸­...")
            
            response = self.session.get(institution_config['url'], timeout=30)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ã§å…¬è¡¨ç‰©ã‚’æ¤œç´¢
            found_publications = []
            
            for pattern in institution_config['search_patterns']:
                # ã‚¿ã‚¤ãƒˆãƒ«è¦ç´ ã‹ã‚‰æ¤œç´¢
                for element in soup.find_all(['h1', 'h2', 'h3', 'h4', 'a', 'span']):
                    text = element.get_text(strip=True)
                    if text and len(text) > 10:
                        if re.search(pattern, text, re.IGNORECASE):
                            date_text = self.extract_date_near_element(element)
                            found_publications.append({
                                'title': text,
                                'date_text': date_text,
                                'url': self.get_element_url(element, institution_config['url']),
                                'extracted_at': datetime.now(self.paris_tz).isoformat()
                            })
                            break
            
            # çµæœã‚’å‡¦ç†
            if found_publications:
                latest = found_publications[0]  # æœ€åˆã«è¦‹ã¤ã‹ã£ãŸã‚‚ã®ã‚’ä½¿ç”¨
                
                # æ—¥ä»˜ã‚’è§£æ
                predicted_date = self.parse_next_date(latest['date_text'], institution_config)
                
                # ãƒ‡ãƒ¼ã‚¿æ›´æ–°
                key = f"{institution_config['institution']}_{institution_config['publication']}"
                self.historical_data[key] = {
                    'institution': institution_config['institution'],
                    'publication': institution_config['publication'],
                    'frequency': institution_config['frequency'],
                    'predicted_next': predicted_date or institution_config['predicted_next'],
                    'last_published': latest.get('date_text', institution_config.get('last_published', '')),
                    'last_title': latest['title'][:100] + '...' if len(latest['title']) > 100 else latest['title'],
                    'last_year_same_period': institution_config.get('last_year_same_period', ''),
                    'last_crawled': datetime.now(self.paris_tz).isoformat(),
                    'source_url': institution_config['url']
                }
                
                logger.info(f"âœ… {institution_config['institution']} æ›´æ–°å®Œäº†")
                return True
            else:
                logger.warning(f"âš ï¸ {institution_config['institution']} - è©²å½“ã™ã‚‹å…¬è¡¨ç‰©ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return False
                
        except Exception as e:
            logger.error(f"âŒ {institution_config['institution']} ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def extract_date_near_element(self, element):
        """è¦ç´ å‘¨è¾ºã‹ã‚‰æ—¥ä»˜ã‚’æŠ½å‡º"""
        date_patterns = [
            r'(\d{1,2}[/-]\d{1,2}[/-]\d{4})',
            r'(\d{4}[/-]\d{1,2}[/-]\d{1,2})',
            r'(january|february|march|april|may|june|july|august|september|october|november|december).*?(\d{4})',
            r'(\d{1,2}).*?(january|february|march|april|may|june|july|august|september|october|november|december).*?(\d{4})'
        ]
        
        # è¦ç´ è‡ªä½“ã®ãƒ†ã‚­ã‚¹ãƒˆ
        text = element.get_text()
        
        # è¦ªè¦ç´ ã®ãƒ†ã‚­ã‚¹ãƒˆã‚‚ç¢ºèª
        if element.parent:
            text += " " + element.parent.get_text()
        
        for pattern in date_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            if matches:
                return matches[0] if isinstance(matches[0], str) else ' '.join(matches[0])
        
        return None

    def get_element_url(self, element, base_url):
        """è¦ç´ ã‹ã‚‰URLã‚’å–å¾—"""
        if element.name == 'a' and element.get('href'):
            href = element.get('href')
            if href.startswith('http'):
                return href
            else:
                return base_url + href
        return base_url

    def parse_next_date(self, date_text, institution_config):
        """æ¬¡å›å…¬è¡¨æ—¥ã‚’äºˆæ¸¬"""
        if not date_text:
            return None
        
        # æ ¼ä»˜ã‘æ©Ÿé–¢ã¯å°‚ç”¨ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ä½¿ç”¨
        if institution_config['institution'] in ['S&P', 'Fitch', 'Moody\'s']:
            country = None
            for c in ['Spain', 'France', 'Italy', 'Portugal']:
                if c.lower() in institution_config['publication'].lower():
                    country = c
                    break
            
            if country and institution_config['institution'] in self.rating_schedule:
                scheduled_date = self.rating_schedule[institution_config['institution']].get(country)
                if scheduled_date:
                    return scheduled_date
        
        # ä¸€èˆ¬çš„ãªäºˆæ¸¬ãƒ­ã‚¸ãƒƒã‚¯
        frequency = institution_config['frequency'].lower()
        if 'annual' in frequency:
            return self.predict_annual_date(date_text)
        elif 'semi-annual' in frequency:
            return self.predict_semi_annual_date(date_text)
        elif 'quarterly' in frequency:
            return self.predict_quarterly_date(date_text)
        else:
            return f"2025å¹´å¾ŒåŠ"  # ä¸å®šæœŸã®å ´åˆ

    def predict_annual_date(self, last_date):
        """å¹´æ¬¡å…¬è¡¨ç‰©ã®æ¬¡å›äºˆæ¸¬"""
        try:
            # ç°¡å˜ãªäºˆæ¸¬ï¼šå‰å¹´åŒæ™‚æœŸ
            return "2025å¹´åŒæ™‚æœŸ"
        except:
            return "2025å¹´å¾ŒåŠ"

    def predict_semi_annual_date(self, last_date):
        """åŠæœŸå…¬è¡¨ç‰©ã®æ¬¡å›äºˆæ¸¬"""
        try:
            # 6ãƒ¶æœˆå¾Œã‚’äºˆæ¸¬
            return "6ãƒ¶æœˆå¾Œäºˆå®š"
        except:
            return "2025å¹´å¾ŒåŠ"

    def predict_quarterly_date(self, last_date):
        """å››åŠæœŸå…¬è¡¨ç‰©ã®æ¬¡å›äºˆæ¸¬"""
        try:
            # 3ãƒ¶æœˆå¾Œã‚’äºˆæ¸¬
            return "3ãƒ¶æœˆå¾Œäºˆå®š"
        except:
            return "2025å¹´å¾ŒåŠ"

    def crawl_all_institutions(self):
        """å…¨æ©Ÿé–¢ã®ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°å®Ÿè¡Œ"""
        logger.info("ğŸš€ å…¨æ©Ÿé–¢ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°é–‹å§‹")
        
        success_count = 0
        total_count = len(self.institutions)
        
        for institution_config in self.institutions:
            if self.crawl_institution(institution_config):
                success_count += 1
            
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
            time.sleep(2)
        
        logger.info(f"âœ… ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°å®Œäº†: {success_count}/{total_count} æˆåŠŸ")
        
        # ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        self.save_data()
        
        # HTMLç”Ÿæˆ
        self.generate_html()

    def generate_html(self):
        """HTMLãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç”Ÿæˆ"""
        try:
            # HTMLãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆèª­ã¿è¾¼ã¿
            if not Path(self.html_template).exists():
                logger.error(f"HTMLãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.html_template}")
                return
            
            with open(self.html_template, 'r', encoding='utf-8') as f:
                html_content = f.read()
            
            # çµ±è¨ˆè¨ˆç®—
            stats = self.calculate_stats()
            
            # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå¤‰æ•°ã‚’ç½®æ›
            html_content = html_content.replace('{{LAST_UPDATE_TIME}}', 
                                              datetime.now(self.paris_tz).strftime('%Y-%m-%d %H:%M:%S (ãƒ‘ãƒªæ™‚é–“)'))
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’å‹•çš„ç”Ÿæˆï¼ˆç¾åœ¨ã®HTMLã®31è¡Œã«å¯¾å¿œï¼‰
            table_rows = self.generate_table_rows()
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«éƒ¨åˆ†ã‚’ç½®æ›ï¼ˆæ—¢å­˜ã®tbodyå†…å®¹ã‚’ç½®ãæ›ãˆï¼‰
            tbody_pattern = r'<tbody>.*?</tbody>'
            new_tbody = f'<tbody>\n{table_rows}\n            </tbody>'
            html_content = re.sub(tbody_pattern, new_tbody, html_content, flags=re.DOTALL)
            
            # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            with open(self.html_output, 'w', encoding='utf-8') as f:
                f.write(html_content)
            
            logger.info(f"âœ… HTMLãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç”Ÿæˆå®Œäº†: {self.html_output}")
            
        except Exception as e:
            logger.error(f"âŒ HTMLç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")

    def calculate_stats(self):
        """çµ±è¨ˆæƒ…å ±è¨ˆç®—"""
        # ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰çµ±è¨ˆã‚’è¨ˆç®—
        # ï¼ˆå®Ÿè£…ç°¡ç•¥åŒ–ï¼‰
        return {
            'today_count': 0,
            'soon_count': 2,  # EBAã®2ä»¶
            'normal_count': 29,
            'irregular_count': 0
        }

    def generate_table_rows(self):
        """ãƒ†ãƒ¼ãƒ–ãƒ«è¡Œã‚’å‹•çš„ç”Ÿæˆ"""
        rows = []
        
        # æ¬¡å›å…¬è¡¨æ—¥é †ã«ã‚½ãƒ¼ãƒˆ
        sorted_data = self.sort_by_next_date()
        
        for i, data in enumerate(sorted_data, 1):
            row = f"""                <tr>
                    <td style="text-align: center; font-weight: bold;">{i}</td>
                    <td class="institution-cell">{data['institution']}</td>
                    <td><strong>{data['publication']}</strong></td>
                    <td>{data['frequency']}</td>
                    <td style="text-align: center;">{data['predicted_next']}</td>
                    <td style="text-align: center;">{data['last_published']}</td>
                    <td>{data['last_title']}</td>
                    <td style="text-align: center;">{data.get('last_year_same_period', '-')}</td>
                </tr>"""
            rows.append(row)
        
        return '\n'.join(rows)

    def sort_by_next_date(self):
        """æ¬¡å›å…¬è¡¨æ—¥é †ã«ã‚½ãƒ¼ãƒˆ"""
        # ç¾åœ¨ã®31æ©Ÿé–¢ãƒ‡ãƒ¼ã‚¿ã‚’æ¬¡å›å…¬è¡¨æ—¥é †ã«ä¸¦ã³æ›¿ãˆ
        # ï¼ˆå®Ÿè£…ç°¡ç•¥åŒ–ï¼šç¾åœ¨ã®HTMLã¨åŒã˜é †åºã‚’ç¶­æŒï¼‰
        
        base_data = []
        for institution_config in self.institutions:
            key = f"{institution_config['institution']}_{institution_config['publication']}"
            if key in self.historical_data:
                base_data.append(self.historical_data[key])
            else:
                # ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨
                base_data.append({
                    'institution': institution_config['institution'],
                    'publication': institution_config['publication'],
                    'frequency': institution_config['frequency'],
                    'predicted_next': institution_config['predicted_next'],
                    'last_published': institution_config.get('last_published', '-'),
                    'last_title': institution_config.get('last_title', '-'),
                    'last_year_same_period': institution_config.get('last_year_same_period', '-')
                })
        
        return base_data

    def schedule_daily_crawling(self):
        """æ¯æ—¥ã®ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«è¨­å®š"""
        # æ¯æœãƒ‘ãƒªæ™‚é–“7æ™‚ã«å®Ÿè¡Œ
        schedule.every().day.at("07:00").do(self.crawl_all_institutions)
        logger.info("ğŸ“… æ¯æ—¥ãƒ‘ãƒªæ™‚é–“7æ™‚ã®ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã‚’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«è¨­å®š")
        
        while True:
            schedule.run_pending()
            time.sleep(60)  # 1åˆ†é–“éš”ã§ãƒã‚§ãƒƒã‚¯

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    monitor = PublicationMonitor()
    
    # åˆå›å®Ÿè¡Œ
    logger.info("ğŸ¯ åˆå›ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°å®Ÿè¡Œ")
    monitor.crawl_all_institutions()
    
    # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å®Ÿè¡Œé–‹å§‹
    logger.info("â° ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å®Ÿè¡Œé–‹å§‹")
    monitor.schedule_daily_crawling()

if __name__ == "__main__":
    main()